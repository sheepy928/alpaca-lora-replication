{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c7c08a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft.git (from -r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 9))\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-3nnseg45\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-3nnseg45\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 86290e9660d24ef0d0cedcf57710da249dd1f2f4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.39.1-py3-none-any.whl (97.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting black\n",
      "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.28.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gradio\n",
      "  Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (5.9.0)\n",
      "Collecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (3.5.1)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (2.0.1)\n",
      "Collecting tokenize-rt>=3.2.0\n",
      "  Downloading tokenize_rt-5.1.0-py2.py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: ipython>=7.8.0 in /opt/conda/lib/python3.10/site-packages (from black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (8.12.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (4.65.0)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (2.29.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 8)) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 10)) (3.9.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (9.4.0)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (2.15.1)\n",
      "Collecting gradio-client>=0.2.7\n",
      "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markupsafe in /opt/conda/lib/python3.10/site-packages (from gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (2.1.1)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (3.1.2)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.0\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair>=4.2.0->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair>=4.2.0->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (4.17.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from altair>=4.2.0->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (4.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (23.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (2.0.4)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (3.0.36)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (5.7.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.2.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
      "INFO: pip is looking at multiple versions of markdown-it-py[linkify] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m135.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (2022.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 7)) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (3.1)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (1.3.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (3.6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 12)) (0.19.3)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (2.0.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.8.0->black->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate->-r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt (line 1)) (1.3.0)\n",
      "Building wheels for collected packages: fire, peft, ffmpy\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=e53c1158aa75cbabb78ada7d71fc23dc7eebf44f7f506bf4dfdd77eda9d92897\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=62052 sha256=b2c4377360fca949d298ae30202239cb313a573ffc506049f92517909b1afe1c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vsorv9sl/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=a4bd25913cc65e84f5dd16e9866e3dbbc3b18b7ed6b288a41bd16fc1e09624c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
      "Successfully built fire peft ffmpy\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pydub, ffmpy, bitsandbytes, appdirs, xxhash, websockets, uc-micro-py, tzdata, tokenize-rt, termcolor, semantic-version, regex, python-multipart, pyparsing, pydantic, pyarrow, pathspec, orjson, mypy-extensions, multidict, mdurl, loralib, kiwisolver, h11, fsspec, frozenlist, fonttools, dill, cycler, contourpy, click, async-timeout, aiofiles, yarl, uvicorn, starlette, pandas, multiprocess, matplotlib, markdown-it-py, linkify-it-py, huggingface-hub, httpcore, fire, black, aiosignal, transformers, mdit-py-plugins, httpx, fastapi, altair, aiohttp, accelerate, peft, gradio-client, gradio, datasets\n",
      "Successfully installed accelerate-0.20.3 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 altair-5.0.1 appdirs-1.4.4 async-timeout-4.0.2 bitsandbytes-0.39.1 black-23.3.0 click-8.1.3 contourpy-1.1.0 cycler-0.11.0 datasets-2.13.1 dill-0.3.6 fastapi-0.98.0 ffmpy-0.3.0 fire-0.5.0 fonttools-4.40.0 frozenlist-1.3.3 fsspec-2023.6.0 gradio-3.35.2 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 kiwisolver-1.4.4 linkify-it-py-2.0.2 loralib-0.1.1 markdown-it-py-2.2.0 matplotlib-3.7.1 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 orjson-3.9.1 pandas-2.0.2 pathspec-0.11.1 peft-0.4.0.dev0 pyarrow-12.0.1 pydantic-1.10.9 pydub-0.25.1 pyparsing-3.1.0 python-multipart-0.0.6 regex-2023.6.3 safetensors-0.3.1 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 termcolor-2.3.0 tokenize-rt-5.1.0 tokenizers-0.13.3 transformers-4.30.2 tzdata-2023.3 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install -q -r https://raw.githubusercontent.com/tloen/alpaca-lora/main/requirements.txt\n",
    "pip install -q scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edd4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p alpaca-lora-replication/alpaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b6823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llm-action'...\n",
      "remote: Enumerating objects: 747, done.        \n",
      "remote: Counting objects: 100% (109/109), done.        \n",
      "remote: Compressing objects: 100% (107/107), done.        \n",
      "remote: Total 747 (delta 58), reused 0 (delta 0), pack-reused 638        \n",
      "Receiving objects: 100% (747/747), 657.50 KiB | 8.65 MiB/s, done.\n",
      "Resolving deltas: 100% (377/377), done.\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/liguodongiot/llm-action.git\n",
    "cp llm-action/alpaca-lora/* alpaca-lora-replication/alpaca-lora/\n",
    "rm -rf llm-action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7de0a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'alpaca-lora'...\n",
      "remote: Enumerating objects: 607, done.        \n",
      "remote: Counting objects: 100% (51/51), done.        \n",
      "remote: Compressing objects: 100% (32/32), done.        \n",
      "remote: Total 607 (delta 28), reused 33 (delta 19), pack-reused 556        \n",
      "Receiving objects: 100% (607/607), 27.78 MiB | 6.23 MiB/s, done.\n",
      "Resolving deltas: 100% (360/360), done.\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/\n",
    "git clone https://github.com/tloen/alpaca-lora.git\n",
    "cp -r alpaca-lora/utils alpaca-lora-replication/alpaca-lora/utils\n",
    "cp -r alpaca-lora/templates alpaca-lora-replication/alpaca-lora/templates\n",
    "rm -rf alpaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8740d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_omklpbyw/none_1ij4w3lk/attempt_0/2/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_omklpbyw/none_1ij4w3lk/attempt_0/3/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_omklpbyw/none_1ij4w3lk/attempt_0/0/error.json')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_omklpbyw/none_1ij4w3lk/attempt_0/1/error.json')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9CUDA SETUP: Detected CUDA version 117\n",
      "\n",
      "CUDA SETUP: Detected CUDA version 117CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: decapoda-research/llama-13b-hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path: yahma/alpaca-cleaned\n",
      "output_dir: output/alpaca-lora-13b\n",
      "batch_size: 72\n",
      "micro_batch_size: 4\n",
      "num_epochs: 2\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "val_set_size: 2000\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n",
      "Loading checkpoint shards: 100%|████████████████| 41/41 [00:30<00:00,  1.37it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|████████████████| 41/41 [00:30<00:00,  1.35it/s]\n",
      "Loading checkpoint shards: 100%|████████████████| 41/41 [00:30<00:00,  1.35it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|████████████████| 41/41 [00:30<00:00,  1.34it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 338.55it/s]\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow\n",
      "Map:   0%|                                     | 0/49760 [00:00<?, ? examples/s]Found cached dataset json (/root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 340.94it/s]\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow\n",
      "Map:   1%|▏                         | 253/49760 [00:00<01:00, 824.85 examples/s]Found cached dataset json (/root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 343.32it/s]\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow\n",
      "Map:   1%|▏                         | 376/49760 [00:00<00:59, 831.06 examples/s]Found cached dataset json (/root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 348.60it/s]\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.05032552357220002\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow\n",
      "                                                                                Map:   0%|                                      | 0/2000 [00:00<?, ? examples/s]Map:   0%|                                      | 0/2000 [00:00<?, ? examples/s]Map:   0%|                                      | 0/2000 [00:00<?, ? examples/s]Map:   0%|                                      | 0/2000 [00:00<?, ? examples/s]Map:  84%|█████████████████████▊    | 1679/2000 [00:02<00:00, 830.68 examples/s]Map:  99%|█████████████████████████▋| 1974/2000 [00:02<00:00, 818.69 examples/s]                                                                                  0%|                                                  | 0/1554 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.8711, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.01}        \n",
      "{'loss': 1.7867, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.03}         \n",
      "{'loss': 1.7414, 'learning_rate': 7.8e-05, 'epoch': 0.04}                       \n",
      "{'loss': 1.4678, 'learning_rate': 0.00010799999999999998, 'epoch': 0.05}        \n",
      "{'loss': 1.209, 'learning_rate': 0.000138, 'epoch': 0.06}                       \n",
      "{'loss': 1.0472, 'learning_rate': 0.000168, 'epoch': 0.08}                      \n",
      "{'loss': 0.9661, 'learning_rate': 0.000198, 'epoch': 0.09}                      \n",
      "{'loss': 0.935, 'learning_rate': 0.00022799999999999999, 'epoch': 0.1}          \n",
      "{'loss': 0.9284, 'learning_rate': 0.000258, 'epoch': 0.12}                      \n",
      "{'loss': 0.8826, 'learning_rate': 0.00028799999999999995, 'epoch': 0.13}        \n",
      "{'loss': 0.8802, 'learning_rate': 0.00029876203576341127, 'epoch': 0.14}        \n",
      "{'loss': 0.8965, 'learning_rate': 0.0002966987620357634, 'epoch': 0.15}         \n",
      "{'loss': 0.8653, 'learning_rate': 0.00029463548830811553, 'epoch': 0.17}        \n",
      "{'loss': 0.8549, 'learning_rate': 0.00029257221458046766, 'epoch': 0.18}        \n",
      "{'loss': 0.8854, 'learning_rate': 0.0002905089408528198, 'epoch': 0.19}         \n",
      "{'loss': 0.8519, 'learning_rate': 0.0002884456671251719, 'epoch': 0.21}         \n",
      "{'loss': 0.8567, 'learning_rate': 0.00028638239339752404, 'epoch': 0.22}        \n",
      "{'loss': 0.8584, 'learning_rate': 0.00028431911966987617, 'epoch': 0.23}        \n",
      "{'loss': 0.868, 'learning_rate': 0.0002822558459422283, 'epoch': 0.24}          \n",
      "{'loss': 0.8586, 'learning_rate': 0.0002801925722145805, 'epoch': 0.26}         \n",
      " 13%|████▉                                 | 200/1554 [20:10<2:15:37,  6.01s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.53it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.50it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.01it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:29,  1.87it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:29,  1.84it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.78it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.77it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.77it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.77it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.77it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:13,  1.77it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.77it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.77it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.77it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8651902079582214, 'eval_runtime': 35.6223, 'eval_samples_per_second': 56.145, 'eval_steps_per_second': 1.769, 'epoch': 0.26}\n",
      " 13%|████▉                                 | 200/1554 [20:46<2:15:37,  6.01s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.84it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8601, 'learning_rate': 0.0002781292984869326, 'epoch': 0.27}         \n",
      "{'loss': 0.8442, 'learning_rate': 0.00027606602475928473, 'epoch': 0.28}        \n",
      "{'loss': 0.8552, 'learning_rate': 0.00027400275103163686, 'epoch': 0.3}         \n",
      "{'loss': 0.8551, 'learning_rate': 0.00027193947730398893, 'epoch': 0.31}        \n",
      "{'loss': 0.8449, 'learning_rate': 0.00026987620357634106, 'epoch': 0.32}        \n",
      "{'loss': 0.8563, 'learning_rate': 0.00026781292984869324, 'epoch': 0.33}        \n",
      "{'loss': 0.8516, 'learning_rate': 0.00026574965612104537, 'epoch': 0.35}        \n",
      "{'loss': 0.838, 'learning_rate': 0.0002636863823933975, 'epoch': 0.36}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8465, 'learning_rate': 0.00026162310866574963, 'epoch': 0.37}        \n",
      "{'loss': 0.8389, 'learning_rate': 0.00025955983493810176, 'epoch': 0.39}        \n",
      "{'loss': 0.8404, 'learning_rate': 0.0002574965612104539, 'epoch': 0.4}          \n",
      "{'loss': 0.8403, 'learning_rate': 0.000255433287482806, 'epoch': 0.41}          \n",
      "{'loss': 0.831, 'learning_rate': 0.00025337001375515814, 'epoch': 0.42}         \n",
      "{'loss': 0.8312, 'learning_rate': 0.00025130674002751027, 'epoch': 0.44}        \n",
      "{'loss': 0.8402, 'learning_rate': 0.0002492434662998624, 'epoch': 0.45}         \n",
      "{'loss': 0.8439, 'learning_rate': 0.0002471801925722146, 'epoch': 0.46}         \n",
      "{'loss': 0.8339, 'learning_rate': 0.0002451169188445667, 'epoch': 0.48}         \n",
      "{'loss': 0.8394, 'learning_rate': 0.00024305364511691883, 'epoch': 0.49}        \n",
      "{'loss': 0.8247, 'learning_rate': 0.00024099037138927096, 'epoch': 0.5}         \n",
      "{'loss': 0.8363, 'learning_rate': 0.0002389270976616231, 'epoch': 0.51}         \n",
      " 26%|█████████▊                            | 400/1554 [40:48<1:55:41,  6.02s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.52it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.49it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.00it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:29,  1.87it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:30,  1.83it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.78it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.77it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.77it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.77it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.76it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:13,  1.76it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.77it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.77it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.77it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.76it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:33<00:01,  1.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8474836349487305, 'eval_runtime': 35.6598, 'eval_samples_per_second': 56.086, 'eval_steps_per_second': 1.767, 'epoch': 0.51}\n",
      " 26%|█████████▊                            | 400/1554 [41:23<1:55:41,  6.02s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.83it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8268, 'learning_rate': 0.00023686382393397522, 'epoch': 0.53}        \n",
      "{'loss': 0.8237, 'learning_rate': 0.00023480055020632737, 'epoch': 0.54}        \n",
      "{'loss': 0.8258, 'learning_rate': 0.0002327372764786795, 'epoch': 0.55}         \n",
      "{'loss': 0.832, 'learning_rate': 0.00023067400275103163, 'epoch': 0.57}         \n",
      "{'loss': 0.8276, 'learning_rate': 0.00022861072902338376, 'epoch': 0.58}        \n",
      "{'loss': 0.8234, 'learning_rate': 0.00022654745529573586, 'epoch': 0.59}        \n",
      "{'loss': 0.8401, 'learning_rate': 0.00022448418156808804, 'epoch': 0.6}         \n",
      "{'loss': 0.8348, 'learning_rate': 0.00022242090784044017, 'epoch': 0.62}        \n",
      "{'loss': 0.8223, 'learning_rate': 0.00022035763411279227, 'epoch': 0.63}        \n",
      "{'loss': 0.8406, 'learning_rate': 0.0002182943603851444, 'epoch': 0.64}         \n",
      "{'loss': 0.8391, 'learning_rate': 0.00021623108665749652, 'epoch': 0.66}        \n",
      "{'loss': 0.8286, 'learning_rate': 0.00021416781292984865, 'epoch': 0.67}        \n",
      "{'loss': 0.8384, 'learning_rate': 0.0002121045392022008, 'epoch': 0.68}         \n",
      "{'loss': 0.8249, 'learning_rate': 0.00021004126547455293, 'epoch': 0.69}        \n",
      "{'loss': 0.8302, 'learning_rate': 0.00020797799174690506, 'epoch': 0.71}        \n",
      "{'loss': 0.8425, 'learning_rate': 0.0002059147180192572, 'epoch': 0.72}         \n",
      "{'loss': 0.8284, 'learning_rate': 0.00020385144429160932, 'epoch': 0.73}        \n",
      "{'loss': 0.8314, 'learning_rate': 0.00020178817056396147, 'epoch': 0.75}        \n",
      "{'loss': 0.8273, 'learning_rate': 0.0001997248968363136, 'epoch': 0.76}         \n",
      "{'loss': 0.8322, 'learning_rate': 0.00019766162310866573, 'epoch': 0.77}        \n",
      " 39%|█████████████▉                      | 600/1554 [1:01:28<1:35:50,  6.03s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.53it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.49it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.00it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:30,  1.87it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:30,  1.83it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.77it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.76it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.77it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.76it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:21,  1.76it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.76it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.76it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.76it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:17<00:18,  1.76it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.76it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:17,  1.76it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.76it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.76it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.76it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.76it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.76it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:13,  1.76it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.76it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.76it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.76it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.76it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.76it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.77it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.76it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.73it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.73it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:30<00:05,  1.72it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.72it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:04,  1.72it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.74it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.75it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.75it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.76it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:34<00:01,  1.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8405300378799438, 'eval_runtime': 35.8241, 'eval_samples_per_second': 55.828, 'eval_steps_per_second': 1.759, 'epoch': 0.77}\n",
      " 39%|█████████████▉                      | 600/1554 [1:02:04<1:35:50,  6.03s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.83it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8367, 'learning_rate': 0.00019559834938101786, 'epoch': 0.78}        \n",
      "{'loss': 0.8174, 'learning_rate': 0.00019353507565336999, 'epoch': 0.8}         \n",
      "{'loss': 0.8205, 'learning_rate': 0.0001914718019257221, 'epoch': 0.81}         \n",
      "{'loss': 0.8304, 'learning_rate': 0.00018940852819807427, 'epoch': 0.82}        \n",
      "{'loss': 0.8274, 'learning_rate': 0.0001875515818431912, 'epoch': 0.84}         \n",
      "{'loss': 0.845, 'learning_rate': 0.00018548830811554332, 'epoch': 0.85}         \n",
      "{'loss': 0.8297, 'learning_rate': 0.00018342503438789544, 'epoch': 0.86}        \n",
      "{'loss': 0.8424, 'learning_rate': 0.0001813617606602476, 'epoch': 0.87}         \n",
      "{'loss': 0.8284, 'learning_rate': 0.00017929848693259973, 'epoch': 0.89}        \n",
      "{'loss': 0.8143, 'learning_rate': 0.00017723521320495186, 'epoch': 0.9}         \n",
      "{'loss': 0.8246, 'learning_rate': 0.00017517193947730396, 'epoch': 0.91}        \n",
      "{'loss': 0.8346, 'learning_rate': 0.00017310866574965608, 'epoch': 0.93}        \n",
      "{'loss': 0.8259, 'learning_rate': 0.00017104539202200827, 'epoch': 0.94}        \n",
      "{'loss': 0.8356, 'learning_rate': 0.00016898211829436037, 'epoch': 0.95}        \n",
      "{'loss': 0.8129, 'learning_rate': 0.0001669188445667125, 'epoch': 0.96}         \n",
      "{'loss': 0.8417, 'learning_rate': 0.00016485557083906462, 'epoch': 0.98}        \n",
      "{'loss': 0.8377, 'learning_rate': 0.00016279229711141675, 'epoch': 0.99}        \n",
      "{'loss': 0.8028, 'learning_rate': 0.00016072902338376888, 'epoch': 1.0}         \n",
      "{'loss': 0.8202, 'learning_rate': 0.00015866574965612103, 'epoch': 1.02}        \n",
      "{'loss': 0.8182, 'learning_rate': 0.00015660247592847316, 'epoch': 1.03}        \n",
      " 51%|██████████████████▌                 | 800/1554 [1:22:08<1:15:17,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.52it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.49it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.00it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:30,  1.86it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:30,  1.83it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:26,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.76it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.77it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.76it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:20,  1.76it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.76it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.76it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:17<00:18,  1.76it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.76it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:17,  1.76it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.76it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.75it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.76it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.76it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.76it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:13,  1.76it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.76it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.76it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.76it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.76it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.76it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.76it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.76it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.76it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.76it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.76it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:30<00:05,  1.76it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.76it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.76it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.76it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:34<00:01,  1.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8377491235733032, 'eval_runtime': 35.7424, 'eval_samples_per_second': 55.956, 'eval_steps_per_second': 1.763, 'epoch': 1.03}\n",
      " 51%|██████████████████▌                 | 800/1554 [1:22:44<1:15:17,  5.99s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.83it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8147, 'learning_rate': 0.0001545392022008253, 'epoch': 1.04}         \n",
      "{'loss': 0.8271, 'learning_rate': 0.00015247592847317742, 'epoch': 1.05}        \n",
      "{'loss': 0.8214, 'learning_rate': 0.00015041265474552955, 'epoch': 1.07}        \n",
      "{'loss': 0.8158, 'learning_rate': 0.0001483493810178817, 'epoch': 1.08}         \n",
      "{'loss': 0.8165, 'learning_rate': 0.00014628610729023383, 'epoch': 1.09}        \n",
      "{'loss': 0.83, 'learning_rate': 0.00014422283356258596, 'epoch': 1.11}          \n",
      "{'loss': 0.8207, 'learning_rate': 0.00014215955983493808, 'epoch': 1.12}        \n",
      "{'loss': 5.9533, 'learning_rate': 0.0001405089408528198, 'epoch': 1.13}         \n",
      "{'loss': 1.5108, 'learning_rate': 0.00013844566712517192, 'epoch': 1.14}        \n",
      "{'loss': 0.8498, 'learning_rate': 0.00013638239339752405, 'epoch': 1.16}        \n",
      "{'loss': 0.8092, 'learning_rate': 0.00013431911966987618, 'epoch': 1.17}        \n",
      "{'loss': 0.8044, 'learning_rate': 0.00013225584594222834, 'epoch': 1.18}        \n",
      "{'loss': 0.8139, 'learning_rate': 0.00013019257221458046, 'epoch': 1.2}         \n",
      "{'loss': 0.8165, 'learning_rate': 0.0001281292984869326, 'epoch': 1.21}         \n",
      "{'loss': 0.8204, 'learning_rate': 0.00012606602475928472, 'epoch': 1.22}        \n",
      "{'loss': 0.8013, 'learning_rate': 0.00012400275103163685, 'epoch': 1.23}        \n",
      "{'loss': 0.819, 'learning_rate': 0.00012193947730398899, 'epoch': 1.25}         \n",
      "{'loss': 0.8185, 'learning_rate': 0.00011987620357634112, 'epoch': 1.26}        \n",
      "{'loss': 0.8181, 'learning_rate': 0.00011781292984869324, 'epoch': 1.27}        \n",
      "{'loss': 0.8193, 'learning_rate': 0.00011574965612104539, 'epoch': 1.29}        \n",
      " 64%|███████████████████████▊             | 1000/1554 [1:42:46<55:27,  6.01s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.53it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.49it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.01it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:29,  1.87it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13%|█████▌                                      | 8/63 [00:03<00:30,  1.83it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.77it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.77it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.77it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.76it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.77it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:13,  1.77it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.77it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.77it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.76it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.77it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:33<00:01,  1.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8347517251968384, 'eval_runtime': 35.6523, 'eval_samples_per_second': 56.097, 'eval_steps_per_second': 1.767, 'epoch': 1.29}\n",
      " 64%|███████████████████████▊             | 1000/1554 [1:43:21<55:27,  6.01s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.84it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8194, 'learning_rate': 0.00011368638239339751, 'epoch': 1.3}         \n",
      "{'loss': 0.7988, 'learning_rate': 0.00011162310866574964, 'epoch': 1.31}        \n",
      "{'loss': 0.8087, 'learning_rate': 0.00010955983493810178, 'epoch': 1.32}        \n",
      "{'loss': 0.8123, 'learning_rate': 0.00010749656121045391, 'epoch': 1.34}        \n",
      "{'loss': 0.8335, 'learning_rate': 0.00010543328748280605, 'epoch': 1.35}        \n",
      "{'loss': 0.8336, 'learning_rate': 0.00010337001375515818, 'epoch': 1.36}        \n",
      "{'loss': 0.8042, 'learning_rate': 0.00010130674002751031, 'epoch': 1.38}        \n",
      "{'loss': 0.8093, 'learning_rate': 9.924346629986245e-05, 'epoch': 1.39}         \n",
      "{'loss': 0.8114, 'learning_rate': 9.718019257221458e-05, 'epoch': 1.4}          \n",
      "{'loss': 0.8125, 'learning_rate': 9.511691884456669e-05, 'epoch': 1.41}         \n",
      "{'loss': 0.8233, 'learning_rate': 9.305364511691883e-05, 'epoch': 1.43}         \n",
      "{'loss': 0.8166, 'learning_rate': 9.099037138927096e-05, 'epoch': 1.44}         \n",
      "{'loss': 0.8014, 'learning_rate': 8.89270976616231e-05, 'epoch': 1.45}          \n",
      "{'loss': 0.7992, 'learning_rate': 8.686382393397523e-05, 'epoch': 1.47}         \n",
      "{'loss': 0.8246, 'learning_rate': 8.480055020632736e-05, 'epoch': 1.48}         \n",
      "{'loss': 0.8243, 'learning_rate': 8.27372764786795e-05, 'epoch': 1.49}          \n",
      "{'loss': 0.803, 'learning_rate': 8.067400275103163e-05, 'epoch': 1.5}           \n",
      "{'loss': 0.8013, 'learning_rate': 7.861072902338376e-05, 'epoch': 1.52}         \n",
      "{'loss': 0.815, 'learning_rate': 7.65474552957359e-05, 'epoch': 1.53}           \n",
      "{'loss': 0.8183, 'learning_rate': 7.448418156808803e-05, 'epoch': 1.54}         \n",
      " 77%|████████████████████████████▌        | 1200/1554 [2:03:24<35:29,  6.02s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.53it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:24,  2.49it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.16it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.01it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.92it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:30,  1.86it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:30,  1.83it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.81it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.80it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:29,  1.79it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:28,  1.78it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24%|██████████▏                                | 15/63 [00:07<00:27,  1.77it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.77it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.76it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.77it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.76it/s]\u001b[A\n",
      " 35%|███████████████                            | 22/63 [00:11<00:23,  1.77it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.76it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:13<00:22,  1.76it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.77it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.77it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.77it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:16<00:18,  1.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.77it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.77it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.77it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.77it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:22<00:12,  1.77it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.77it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.77it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.77it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:26<00:09,  1.77it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.77it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.77it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:29<00:05,  1.77it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.77it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.77it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.77it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:33<00:01,  1.77it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.8479226231575012, 'eval_runtime': 35.6658, 'eval_samples_per_second': 56.076, 'eval_steps_per_second': 1.766, 'epoch': 1.54}\n",
      " 77%|████████████████████████████▌        | 1200/1554 [2:03:59<35:29,  6.02s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:35<00:00,  1.84it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.8382, 'learning_rate': 7.242090784044015e-05, 'epoch': 1.56}         \n",
      "{'loss': 1.4395, 'learning_rate': 7.03576341127923e-05, 'epoch': 1.57}          \n",
      "{'loss': 0.8874, 'learning_rate': 6.829436038514442e-05, 'epoch': 1.58}         \n",
      "{'loss': 0.9516, 'learning_rate': 6.623108665749655e-05, 'epoch': 1.59}         \n",
      "{'loss': 0.9035, 'learning_rate': 6.416781292984869e-05, 'epoch': 1.61}         \n",
      "{'loss': 0.9012, 'learning_rate': 6.210453920220082e-05, 'epoch': 1.62}         \n",
      "{'loss': 0.9188, 'learning_rate': 6.004126547455295e-05, 'epoch': 1.63}         \n",
      "{'loss': 0.9967, 'learning_rate': 5.797799174690508e-05, 'epoch': 1.65}         \n",
      "{'loss': 0.9389, 'learning_rate': 5.591471801925722e-05, 'epoch': 1.66}         \n",
      "{'loss': 0.9694, 'learning_rate': 5.385144429160935e-05, 'epoch': 1.67}         \n",
      "{'loss': 0.9749, 'learning_rate': 5.178817056396148e-05, 'epoch': 1.68}         \n",
      "{'loss': 1.1335, 'learning_rate': 4.9724896836313614e-05, 'epoch': 1.7}         \n",
      "{'loss': 1.0597, 'learning_rate': 4.766162310866575e-05, 'epoch': 1.71}         \n",
      "{'loss': 1.1132, 'learning_rate': 4.5598349381017884e-05, 'epoch': 1.72}        \n",
      "{'loss': 1.0523, 'learning_rate': 4.3535075653370005e-05, 'epoch': 1.74}        \n",
      "{'loss': 1.1123, 'learning_rate': 4.147180192572214e-05, 'epoch': 1.75}         \n",
      "{'loss': 1.0786, 'learning_rate': 3.9408528198074274e-05, 'epoch': 1.76}        \n",
      "{'loss': 1.191, 'learning_rate': 3.734525447042641e-05, 'epoch': 1.77}          \n",
      "{'loss': 1.0926, 'learning_rate': 3.528198074277854e-05, 'epoch': 1.79}         \n",
      "{'loss': 1.1108, 'learning_rate': 3.321870701513067e-05, 'epoch': 1.8}          \n",
      " 90%|█████████████████████████████████▎   | 1400/1554 [2:23:59<15:23,  6.00s/it]\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▍                                          | 2/63 [00:00<00:17,  3.58it/s]\u001b[A\n",
      "  5%|██                                          | 3/63 [00:01<00:23,  2.52it/s]\u001b[A\n",
      "  6%|██▊                                         | 4/63 [00:01<00:27,  2.18it/s]\u001b[A\n",
      "  8%|███▍                                        | 5/63 [00:02<00:28,  2.03it/s]\u001b[A\n",
      " 10%|████▏                                       | 6/63 [00:02<00:29,  1.94it/s]\u001b[A\n",
      " 11%|████▉                                       | 7/63 [00:03<00:29,  1.89it/s]\u001b[A\n",
      " 13%|█████▌                                      | 8/63 [00:03<00:29,  1.85it/s]\u001b[A\n",
      " 14%|██████▎                                     | 9/63 [00:04<00:29,  1.83it/s]\u001b[A\n",
      " 16%|██████▊                                    | 10/63 [00:05<00:29,  1.82it/s]\u001b[A\n",
      " 17%|███████▌                                   | 11/63 [00:05<00:28,  1.81it/s]\u001b[A\n",
      " 19%|████████▏                                  | 12/63 [00:06<00:28,  1.80it/s]\u001b[A\n",
      " 21%|████████▊                                  | 13/63 [00:06<00:27,  1.80it/s]\u001b[A\n",
      " 22%|█████████▌                                 | 14/63 [00:07<00:27,  1.80it/s]\u001b[A\n",
      " 24%|██████████▏                                | 15/63 [00:07<00:26,  1.80it/s]\u001b[A\n",
      " 25%|██████████▉                                | 16/63 [00:08<00:26,  1.79it/s]\u001b[A\n",
      " 27%|███████████▌                               | 17/63 [00:08<00:25,  1.79it/s]\u001b[A\n",
      " 29%|████████████▎                              | 18/63 [00:09<00:25,  1.79it/s]\u001b[A\n",
      " 30%|████████████▉                              | 19/63 [00:10<00:24,  1.79it/s]\u001b[A\n",
      " 32%|█████████████▋                             | 20/63 [00:10<00:24,  1.79it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 21/63 [00:11<00:23,  1.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35%|███████████████                            | 22/63 [00:11<00:22,  1.79it/s]\u001b[A\n",
      " 37%|███████████████▋                           | 23/63 [00:12<00:22,  1.79it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 24/63 [00:12<00:21,  1.78it/s]\u001b[A\n",
      " 40%|█████████████████                          | 25/63 [00:13<00:21,  1.78it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 26/63 [00:13<00:20,  1.78it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 27/63 [00:14<00:20,  1.78it/s]\u001b[A\n",
      " 44%|███████████████████                        | 28/63 [00:15<00:19,  1.79it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 29/63 [00:15<00:19,  1.79it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 30/63 [00:16<00:18,  1.79it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 31/63 [00:16<00:17,  1.79it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 32/63 [00:17<00:17,  1.79it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 33/63 [00:17<00:16,  1.79it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 34/63 [00:18<00:16,  1.79it/s]\u001b[A\n",
      " 56%|███████████████████████▉                   | 35/63 [00:19<00:15,  1.79it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 36/63 [00:19<00:15,  1.79it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 37/63 [00:20<00:14,  1.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▉                 | 38/63 [00:20<00:14,  1.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 39/63 [00:21<00:13,  1.79it/s]\u001b[A\n",
      " 63%|███████████████████████████▎               | 40/63 [00:21<00:12,  1.79it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 41/63 [00:22<00:12,  1.79it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 42/63 [00:22<00:11,  1.79it/s]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 43/63 [00:23<00:11,  1.79it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 44/63 [00:24<00:10,  1.79it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 45/63 [00:24<00:10,  1.79it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 46/63 [00:25<00:09,  1.79it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 47/63 [00:25<00:08,  1.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 48/63 [00:26<00:08,  1.79it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 49/63 [00:26<00:07,  1.79it/s]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 50/63 [00:27<00:07,  1.79it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 51/63 [00:27<00:06,  1.79it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▍       | 52/63 [00:28<00:06,  1.79it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 53/63 [00:29<00:05,  1.78it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 54/63 [00:29<00:05,  1.79it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 55/63 [00:30<00:04,  1.79it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 56/63 [00:30<00:03,  1.79it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▉    | 57/63 [00:31<00:03,  1.79it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 58/63 [00:31<00:02,  1.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 59/63 [00:32<00:02,  1.79it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:33<00:01,  1.78it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 61/63 [00:33<00:01,  1.78it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▎| 62/63 [00:34<00:00,  1.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.0643361806869507, 'eval_runtime': 35.2715, 'eval_samples_per_second': 56.703, 'eval_steps_per_second': 1.786, 'epoch': 1.8}\n",
      " 90%|█████████████████████████████████▎   | 1400/1554 [2:24:34<15:23,  6.00s/it]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:34<00:00,  1.86it/s]\u001b[A\n",
      "                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.0755, 'learning_rate': 3.11554332874828e-05, 'epoch': 1.81}          \n",
      "{'loss': 1.1625, 'learning_rate': 2.9092159559834938e-05, 'epoch': 1.83}        \n",
      "{'loss': 1.0989, 'learning_rate': 2.7028885832187065e-05, 'epoch': 1.84}        \n",
      "{'loss': 1.1059, 'learning_rate': 2.49656121045392e-05, 'epoch': 1.85}          \n",
      "{'loss': 1.1172, 'learning_rate': 2.290233837689133e-05, 'epoch': 1.86}         \n",
      "{'loss': 1.1605, 'learning_rate': 2.0839064649243466e-05, 'epoch': 1.88}        \n",
      "{'loss': 1.0872, 'learning_rate': 1.8982118294360387e-05, 'epoch': 1.89}        \n",
      "{'loss': 1.1527, 'learning_rate': 1.6918844566712515e-05, 'epoch': 1.9}         \n",
      "{'loss': 1.1058, 'learning_rate': 1.4855570839064648e-05, 'epoch': 1.92}        \n",
      "{'loss': 1.1608, 'learning_rate': 1.279229711141678e-05, 'epoch': 1.93}         \n",
      "{'loss': 1.2098, 'learning_rate': 1.0729023383768912e-05, 'epoch': 1.94}        \n",
      "{'loss': 1.1514, 'learning_rate': 8.665749656121045e-06, 'epoch': 1.95}         \n",
      "{'loss': 1.1621, 'learning_rate': 6.602475928473177e-06, 'epoch': 1.97}         \n",
      "{'loss': 1.1734, 'learning_rate': 4.539202200825309e-06, 'epoch': 1.98}         \n",
      "{'loss': 1.1364, 'learning_rate': 2.4759284731774413e-06, 'epoch': 1.99}        \n",
      "{'train_runtime': 9599.0687, 'train_samples_per_second': 10.368, 'train_steps_per_second': 0.162, 'train_loss': 0.9534891563032585, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████| 1554/1554 [2:39:57<00:00,  6.18s/it]\n",
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n",
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/alpaca-lora-replication/alpaca-lora\n",
    "torchrun --nproc_per_node=4 finetune.py \\\n",
    "    --base_model 'huggyllama/llama-13b-hf' \\\n",
    "    --data_path 'yahma/alpaca-cleaned' \\\n",
    "    --output_dir 'output/alpaca-lora-13b' \\\n",
    "    --batch_size 72 \\\n",
    "    --micro_batch_size 4 \\\n",
    "    --num_epochs 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
